{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "XCNv5MBbbFWs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "j8RTaadkkrLL"
   },
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv('/content/drive/MyDrive/R03_25.unknown')\n",
    "dataset = pd.read_csv(\"./R03_50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "PpeohlXGzDaG"
   },
   "outputs": [],
   "source": [
    "dataset.drop(columns={\"Unnamed: 0\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "vgdSc6tPzA9B",
    "outputId": "739ce03d-1a4d-4782-a7cd-bb6d44c010ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fc5.</th>\n",
       "      <th>Fc3.</th>\n",
       "      <th>Fc1.</th>\n",
       "      <th>Fcz.</th>\n",
       "      <th>Fc2.</th>\n",
       "      <th>Fc4.</th>\n",
       "      <th>Fc6.</th>\n",
       "      <th>C5..</th>\n",
       "      <th>C3..</th>\n",
       "      <th>C1..</th>\n",
       "      <th>...</th>\n",
       "      <th>Po7.</th>\n",
       "      <th>Po3.</th>\n",
       "      <th>Poz.</th>\n",
       "      <th>Po4.</th>\n",
       "      <th>Po8.</th>\n",
       "      <th>O1..</th>\n",
       "      <th>Oz..</th>\n",
       "      <th>O2..</th>\n",
       "      <th>Iz..</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-57.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-49.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-149.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-55.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>-153.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-148.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-87.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>-119.0</td>\n",
       "      <td>-118.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fc5.  Fc3.  Fc1.  Fcz.  Fc2.  Fc4.  Fc6.  C5..  C3..  C1..  ...  Po7.  \\\n",
       "0 -57.0 -13.0 -15.0 -12.0 -13.0  -8.0 -40.0 -54.0 -12.0 -14.0  ... -38.0   \n",
       "1 -49.0 -11.0 -10.0 -12.0 -19.0 -24.0 -58.0 -51.0 -19.0 -23.0  ... -55.0   \n",
       "2 -55.0 -17.0 -16.0 -19.0 -24.0 -29.0 -66.0 -61.0 -30.0 -36.0  ... -63.0   \n",
       "3 -73.0 -42.0 -40.0 -37.0 -37.0 -40.0 -71.0 -78.0 -53.0 -53.0  ... -52.0   \n",
       "4 -87.0 -53.0 -52.0 -51.0 -45.0 -43.0 -71.0 -87.0 -65.0 -64.0  ... -82.0   \n",
       "\n",
       "   Po3.   Poz.   Po4.   Po8.  O1..  Oz..   O2..  Iz..  label  \n",
       "0 -42.0  -68.0  -76.0 -103.0 -51.0 -56.0 -124.0 -28.0      0  \n",
       "1 -63.0  -82.0  -87.0  -99.0 -59.0 -70.0 -149.0 -40.0      0  \n",
       "2 -72.0  -91.0  -92.0  -91.0 -67.0 -77.0 -153.0 -37.0      0  \n",
       "3 -66.0 -100.0 -105.0 -105.0 -67.0 -72.0 -148.0 -26.0      0  \n",
       "4 -90.0 -117.0 -119.0 -118.0 -75.0 -82.0 -161.0 -35.0      0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "VvwA8JFQuHWA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset.drop(columns={\"label\"})\n",
    "Y = dataset['label']\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled =  scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "jYyrljp0u1iv"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "NwV4a5n7vApk"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uE79f8lQvYqB",
    "outputId": "1824f448-88b7-47b3-8b40-982f1b2276f4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.9332 - acc: 0.5578\n",
      "epoch 2/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.8064 - acc: 0.6288\n",
      "epoch 3/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.7522 - acc: 0.6572\n",
      "epoch 4/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.7196 - acc: 0.6735\n",
      "epoch 5/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6985 - acc: 0.6833\n",
      "epoch 6/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6814 - acc: 0.6922\n",
      "epoch 7/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6695 - acc: 0.6975\n",
      "epoch 8/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6591 - acc: 0.7025\n",
      "epoch 9/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.6517 - acc: 0.7064\n",
      "epoch 10/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 116us/sample - loss: 0.6447 - acc: 0.7093\n",
      "epoch 11/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.6402 - acc: 0.7120\n",
      "epoch 12/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6350 - acc: 0.7144\n",
      "epoch 13/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6313 - acc: 0.7167\n",
      "epoch 14/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6277 - acc: 0.7181\n",
      "epoch 15/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.6239 - acc: 0.7197\n",
      "epoch 16/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.6211 - acc: 0.7208\n",
      "epoch 17/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6182 - acc: 0.7225\n",
      "epoch 18/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6157 - acc: 0.7237\n",
      "epoch 19/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6135 - acc: 0.7247\n",
      "epoch 20/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6117 - acc: 0.7258\n",
      "epoch 21/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6092 - acc: 0.7267\n",
      "epoch 22/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6081 - acc: 0.7271\n",
      "epoch 23/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.6072 - acc: 0.7278\n",
      "epoch 24/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 93s 118us/sample - loss: 0.6049 - acc: 0.7288\n",
      "epoch 25/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.6039 - acc: 0.7296\n",
      "epoch 26/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6032 - acc: 0.7298\n",
      "epoch 27/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6021 - acc: 0.7306\n",
      "epoch 28/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6012 - acc: 0.7311\n",
      "epoch 29/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6002 - acc: 0.7312\n",
      "epoch 30/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6000 - acc: 0.7317\n",
      "epoch 31/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5997 - acc: 0.7324\n",
      "epoch 32/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5991 - acc: 0.7323\n",
      "epoch 33/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5988 - acc: 0.7320\n",
      "epoch 34/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5980 - acc: 0.7325\n",
      "epoch 35/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5975 - acc: 0.7332\n",
      "epoch 36/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.5972 - acc: 0.7333\n",
      "epoch 37/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5969 - acc: 0.7334\n",
      "epoch 38/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5956 - acc: 0.7338\n",
      "epoch 39/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5962 - acc: 0.7333\n",
      "epoch 40/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5954 - acc: 0.7340\n",
      "epoch 41/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5951 - acc: 0.7345\n",
      "epoch 42/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5949 - acc: 0.7347\n",
      "epoch 43/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5955 - acc: 0.7345\n",
      "epoch 44/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5952 - acc: 0.7346\n",
      "epoch 45/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5958 - acc: 0.7338\n",
      "epoch 46/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 116us/sample - loss: 0.5955 - acc: 0.7345\n",
      "epoch 47/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5954 - acc: 0.7342\n",
      "epoch 48/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.5957 - acc: 0.7341\n",
      "epoch 49/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5955 - acc: 0.7349\n",
      "epoch 50/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5954 - acc: 0.7344\n",
      "epoch 51/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5957 - acc: 0.7348\n",
      "epoch 52/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5961 - acc: 0.7345\n",
      "epoch 53/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 115us/sample - loss: 0.5955 - acc: 0.7347\n",
      "epoch 54/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.5957 - acc: 0.7348\n",
      "epoch 55/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5978 - acc: 0.7339\n",
      "epoch 56/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 112us/sample - loss: 0.5974 - acc: 0.7342\n",
      "epoch 57/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.5994 - acc: 0.7331\n",
      "epoch 58/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6001 - acc: 0.7330\n",
      "epoch 59/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6014 - acc: 0.7329\n",
      "epoch 60/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 112us/sample - loss: 0.6016 - acc: 0.7327\n",
      "epoch 61/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 111us/sample - loss: 0.6024 - acc: 0.7325\n",
      "epoch 62/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6038 - acc: 0.7321\n",
      "epoch 63/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6047 - acc: 0.7315\n",
      "epoch 64/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6061 - acc: 0.7309\n",
      "epoch 65/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 115us/sample - loss: 0.6056 - acc: 0.7313\n",
      "epoch 66/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6075 - acc: 0.7306\n",
      "epoch 67/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 112us/sample - loss: 0.6074 - acc: 0.7308\n",
      "epoch 68/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 111us/sample - loss: 0.6108 - acc: 0.7293\n",
      "epoch 69/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6100 - acc: 0.7296\n",
      "epoch 70/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6141 - acc: 0.7280\n",
      "epoch 71/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6130 - acc: 0.7285\n",
      "epoch 72/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6151 - acc: 0.7282\n",
      "epoch 73/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 112us/sample - loss: 0.6158 - acc: 0.7277\n",
      "epoch 74/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 112us/sample - loss: 0.6156 - acc: 0.7277\n",
      "epoch 75/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6166 - acc: 0.7275\n",
      "epoch 76/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6177 - acc: 0.7269\n",
      "epoch 77/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6239 - acc: 0.7245\n",
      "epoch 78/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6231 - acc: 0.7250\n",
      "epoch 79/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6221 - acc: 0.7253\n",
      "epoch 80/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 112us/sample - loss: 0.6219 - acc: 0.7254\n",
      "epoch 81/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6213 - acc: 0.7255\n",
      "epoch 82/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6232 - acc: 0.7252\n",
      "epoch 83/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 115us/sample - loss: 0.6253 - acc: 0.7245\n",
      "epoch 84/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 112us/sample - loss: 0.6243 - acc: 0.7251\n",
      "epoch 85/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 112us/sample - loss: 0.6245 - acc: 0.7256\n",
      "epoch 86/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6248 - acc: 0.7251\n",
      "epoch 87/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6274 - acc: 0.7240\n",
      "epoch 88/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6274 - acc: 0.7236\n",
      "epoch 89/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 114us/sample - loss: 0.6284 - acc: 0.7235\n",
      "epoch 90/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6300 - acc: 0.7230\n",
      "epoch 91/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 90s 113us/sample - loss: 0.6291 - acc: 0.7234\n",
      "epoch 92/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 112us/sample - loss: 0.6354 - acc: 0.7217\n",
      "epoch 93/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 111us/sample - loss: 0.6310 - acc: 0.7230\n",
      "epoch 94/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 113us/sample - loss: 0.6314 - acc: 0.7226\n",
      "epoch 95/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 91s 115us/sample - loss: 0.6377 - acc: 0.7200\n",
      "epoch 96/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 112us/sample - loss: 0.6390 - acc: 0.7198\n",
      "epoch 97/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 112us/sample - loss: 0.6394 - acc: 0.7202\n",
      "epoch 98/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 111us/sample - loss: 0.6452 - acc: 0.7184\n",
      "epoch 99/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 88s 111us/sample - loss: 0.6503 - acc: 0.7194\n",
      "epoch 100/100\n",
      "Train on 788761 samples\n",
      "788761/788761 [==============================] - 89s 112us/sample - loss: 0.6424 - acc: 0.7196\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1}/100\")\n",
    "    history = model.fit(X_train_reshaped, Y_train, epochs=1)  \n",
    "    train_accuracies.append(history.history['acc'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-c_muaEvcqw",
    "outputId": "c2933036-9324-4c71-a7f9-4865568ba6aa"
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, Y_test, verbose=0)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDnQ4lGl75xV"
   },
   "outputs": [],
   "source": [
    "print(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [i for i in range(1,epochs+1)]\n",
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(epochs_list, train_accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy vs. Epochs')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 101))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
